import os
import cv2
import numpy as np
import albumentations as A
import shutil
from pathlib import Path
import json
import random
from collections import Counter, defaultdict
import yaml
from ultralytics import YOLO
import torch
import time

class FinalSnackTrainer:
    """ìµœì¢… ê³¼ì íƒì§€ ëª¨ë¸ í›ˆë ¨ê¸° - ëª¨ë“  ë¬¸ì œ í•´ê²°"""
    
    def __init__(self, dataset_path, target_instances=600, model_size='m', epochs=80, exclude_classes=None):
        self.dataset_path = Path(dataset_path)
        self.target_instances = target_instances
        self.model_size = model_size
        self.epochs = epochs
        self.exclude_classes = exclude_classes or [1]  # Backside ì œì™¸
        
        # ì›ë³¸ data.yamlì—ì„œ í™•ì¸ëœ ì •í™•í•œ 24ê°œ í´ë˜ìŠ¤
        self.original_classes = [
            'Alsaeuchip',           # 0
            'Backside',             # 1 â† ì œì™¸í•  í´ë˜ìŠ¤
            'BananaKick',           # 2
            'CaramelCornMaple',     # 3
            'Cheetos',              # 4
            'CornChips',            # 5
            'Gamjakkang',           # 6
            'Jjanggu',              # 7
            'JollyPong',            # 8
            'Kkobugchip',           # 9
            'Kkochgelang',          # 10
            'Kkulkkwabaegi',        # 11
            'KokkalCorn',           # 12
            'Koncho',               # 13
            'Matdongsan',           # 14
            'Ogamja',               # 15
            'Pocachip_Onion',       # 16
            'Pocachip_Original',    # 17
            'Postick',              # 18
            'Saeukkang',            # 19
            'Sunchip',              # 20
            'Swingchip',            # 21
            'Yangpaling',           # 22
            'konchi'                # 23
        ]
        
        # ì œì™¸ í›„ í´ë˜ìŠ¤ (23ê°œ)
        self.valid_classes = [self.original_classes[i] for i in range(24) if i not in self.exclude_classes]
        
        # ID ë§¤í•‘ í…Œì´ë¸” (ì›ë³¸ ID â†’ ìƒˆ ID)
        self.id_mapping = {}
        new_id = 0
        for old_id in range(24):
            if old_id not in self.exclude_classes:
                self.id_mapping[old_id] = new_id
                new_id += 1
        
        self.max_instances = int(target_instances * 1.2)  # ìƒí•œì„ 
        
        print(f"ğŸ¯ ìµœì¢… ê³¼ì íƒì§€ ëª¨ë¸ í›ˆë ¨ê¸°")
        print(f"ğŸ“Š ì›ë³¸: 24ê°œ í´ë˜ìŠ¤ â†’ ì •ë¦¬ í›„: {len(self.valid_classes)}ê°œ í´ë˜ìŠ¤")
        print(f"ğŸš« ì œì™¸: {[self.original_classes[i] for i in self.exclude_classes]}")
        print(f"ğŸ¯ ëª©í‘œ: {target_instances}ê°œ/í´ë˜ìŠ¤, ìƒí•œì„ : {self.max_instances}ê°œ")
        print(f"ğŸ“‹ ID ë§¤í•‘: 0-23 â†’ 0-{len(self.valid_classes)-1}")

    def scan_and_fix_all_labels(self):
        """ëª¨ë“  ë¼ë²¨ íŒŒì¼ ìŠ¤ìº” ë° ìˆ˜ì •"""
        print("\nğŸ”§ ëª¨ë“  ë¼ë²¨ íŒŒì¼ ìŠ¤ìº” ë° ìˆ˜ì • ì¤‘...")
        
        # ë°±ì—… ìƒì„±
        backup_dir = self.dataset_path / "labels_backup_final"
        if not backup_dir.exists():
            print("ğŸ’¾ ë¼ë²¨ ë°±ì—… ìƒì„± ì¤‘...")
            backup_dir.mkdir()
            for split in ["train", "valid", "test"]:
                src_dir = self.dataset_path / split / "labels"
                if src_dir.exists():
                    dst_dir = backup_dir / split
                    shutil.copytree(src_dir, dst_dir)
        
        total_files = 0
        total_fixed = 0
        total_removed_labels = 0
        total_removed_files = 0
        
        # ê° split ì²˜ë¦¬
        for split in ["train", "valid", "test"]:
            labels_dir = self.dataset_path / split / "labels"
            images_dir = self.dataset_path / split / "images"
            
            if not labels_dir.exists():
                continue
            
            print(f"\nğŸ“ {split} ì²˜ë¦¬ ì¤‘...")
            split_files = 0
            split_fixed = 0
            split_removed_labels = 0
            split_removed_files = 0
            
            for label_file in labels_dir.glob("*.txt"):
                total_files += 1
                split_files += 1
                
                try:
                    with open(label_file, 'r') as f:
                        lines = f.readlines()
                    
                    new_lines = []
                    file_changed = False
                    
                    for line in lines:
                        line = line.strip()
                        if line:
                            parts = line.split()
                            if len(parts) >= 5:
                                try:
                                    old_class_id = int(parts[0])
                                    
                                    # ë²”ìœ„ ì²´í¬ (0-23ë§Œ ìœ íš¨)
                                    if old_class_id < 0 or old_class_id > 23:
                                        split_removed_labels += 1
                                        file_changed = True
                                        continue
                                    
                                    # ì œì™¸ í´ë˜ìŠ¤ ì œê±°
                                    if old_class_id in self.exclude_classes:
                                        split_removed_labels += 1
                                        file_changed = True
                                        continue
                                    
                                    # ID ì¬ë§¤í•‘
                                    new_class_id = self.id_mapping[old_class_id]
                                    parts[0] = str(new_class_id)
                                    new_lines.append(' '.join(parts) + '\n')
                                    
                                    if new_class_id != old_class_id:
                                        file_changed = True
                                        
                                except ValueError:
                                    split_removed_labels += 1
                                    file_changed = True
                                    continue
                    
                    # íŒŒì¼ ì²˜ë¦¬
                    if len(new_lines) == 0:
                        # ë¹ˆ íŒŒì¼ - ì´ë¯¸ì§€ì™€ ë¼ë²¨ ëª¨ë‘ ì‚­ì œ
                        image_name = label_file.stem
                        for ext in ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG']:
                            image_file = images_dir / (image_name + ext)
                            if image_file.exists():
                                image_file.unlink()
                                break
                        label_file.unlink()
                        split_removed_files += 1
                        
                    elif file_changed:
                        # íŒŒì¼ ì—…ë°ì´íŠ¸
                        with open(label_file, 'w') as f:
                            f.writelines(new_lines)
                        split_fixed += 1
                
                except Exception as e:
                    print(f"      âš ï¸ ì˜¤ë¥˜ {label_file.name}: {e}")
            
            print(f"   âœ… {split} ì™„ë£Œ: {split_files}ê°œ íŒŒì¼, {split_fixed}ê°œ ìˆ˜ì •, {split_removed_labels}ê°œ ë¼ë²¨ ì œê±°, {split_removed_files}ê°œ íŒŒì¼ ì‚­ì œ")
            
            total_fixed += split_fixed
            total_removed_labels += split_removed_labels
            total_removed_files += split_removed_files
        
        print(f"\nğŸ‰ ë¼ë²¨ ì •ë¦¬ ì™„ë£Œ!")
        print(f"   ì´ íŒŒì¼: {total_files}ê°œ")
        print(f"   ìˆ˜ì •ëœ íŒŒì¼: {total_fixed}ê°œ")
        print(f"   ì œê±°ëœ ë¼ë²¨: {total_removed_labels}ê°œ")
        print(f"   ì‚­ì œëœ íŒŒì¼: {total_removed_files}ê°œ")
        print(f"   ë°±ì—… ìœ„ì¹˜: {backup_dir}")
        
        return True

    def validate_final_labels(self):
        """ìµœì¢… ë¼ë²¨ ê²€ì¦"""
        print("\nğŸ” ìµœì¢… ë¼ë²¨ ê²€ì¦ ì¤‘...")
        
        all_class_ids = set()
        total_files = 0
        total_labels = 0
        error_count = 0
        
        for split in ["train", "valid", "test"]:
            labels_dir = self.dataset_path / split / "labels"
            if not labels_dir.exists():
                continue
            
            for label_file in labels_dir.glob("*.txt"):
                total_files += 1
                
                try:
                    with open(label_file, 'r') as f:
                        for line in f:
                            line = line.strip()
                            if line:
                                parts = line.split()
                                if len(parts) >= 5:
                                    class_id = int(parts[0])
                                    all_class_ids.add(class_id)
                                    total_labels += 1
                                    
                                    # ë²”ìœ„ ì²´í¬
                                    if class_id < 0 or class_id >= len(self.valid_classes):
                                        error_count += 1
                                        print(f"      âŒ ì˜ëª»ëœ ID {class_id} in {label_file.name}")
                
                except Exception as e:
                    error_count += 1
                    print(f"      âŒ íŒŒì¼ ì˜¤ë¥˜ {label_file.name}: {e}")
        
        print(f"ğŸ“Š ìµœì¢… ê²€ì¦ ê²°ê³¼:")
        print(f"   ì´ íŒŒì¼: {total_files}ê°œ")
        print(f"   ì´ ë¼ë²¨: {total_labels}ê°œ")
        print(f"   ë°œê²¬ëœ í´ë˜ìŠ¤ ID: {sorted(all_class_ids)}")
        print(f"   ìœ íš¨ ë²”ìœ„: 0-{len(self.valid_classes)-1}")
        print(f"   ì˜¤ë¥˜ ìˆ˜: {error_count}ê°œ")
        
        if error_count == 0:
            print(f"   âœ… ëª¨ë“  ë¼ë²¨ì´ ì˜¬ë°”ë¦„!")
            return True
        else:
            print(f"   âŒ {error_count}ê°œ ë¬¸ì œ ë°œê²¬")
            return False

    def create_final_yaml(self):
        """ìµœì¢… YAML íŒŒì¼ ìƒì„±"""
        print("\nğŸ“ ìµœì¢… YAML íŒŒì¼ ìƒì„± ì¤‘...")
        
        # ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš©
        dataset_abs_path = self.dataset_path.resolve()
        
        # ê²½ë¡œ ê²€ì¦
        train_path = dataset_abs_path / "train" / "images"
        val_path = dataset_abs_path / "valid" / "images" 
        test_path = dataset_abs_path / "test" / "images"
        
        print(f"ğŸ“ ê²½ë¡œ ê²€ì¦:")
        print(f"   ë°ì´í„°ì…‹: {dataset_abs_path}")
        print(f"   Train: {'âœ…' if train_path.exists() else 'âŒ'}")
        print(f"   Valid: {'âœ…' if val_path.exists() else 'âŒ'}")
        print(f"   Test: {'âœ…' if test_path.exists() else 'âŒ'}")
        
        # YAML ë‚´ìš© ìƒì„±
        yaml_content = f"""path: {dataset_abs_path}/
train: train/images
val: valid/images
test: test/images

nc: {len(self.valid_classes)}
names: {self.valid_classes}

# ìµœì¢… ì •ë¦¬ëœ ê³¼ì íƒì§€ ë°ì´í„°
# ì›ë³¸: 24ê°œ í´ë˜ìŠ¤ â†’ ì •ë¦¬ í›„: {len(self.valid_classes)}ê°œ í´ë˜ìŠ¤
# ì œì™¸ëœ í´ë˜ìŠ¤: {[self.original_classes[i] for i in self.exclude_classes]}
# ëª©í‘œ: {self.target_instances}ê°œ/í´ë˜ìŠ¤, ìƒí•œì„ : {self.max_instances}ê°œ/í´ë˜ìŠ¤
# ìƒì„± ì¼ì‹œ: {time.strftime('%Y-%m-%d %H:%M:%S')}
"""
        
        # í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ì— ì €ì¥
        yaml_path = Path.cwd() / "final_snack_data.yaml"
        
        with open(yaml_path, 'w', encoding='utf-8') as f:
            f.write(yaml_content)
        
        print(f"âœ… YAML íŒŒì¼ ìƒì„± ì™„ë£Œ:")
        print(f"   íŒŒì¼: {yaml_path}")
        print(f"   í´ë˜ìŠ¤ ìˆ˜: {len(self.valid_classes)}")
        
        # YAML ê²€ì¦
        try:
            with open(yaml_path, 'r') as f:
                yaml_data = yaml.safe_load(f)
            print(f"   âœ… YAML ë¬¸ë²• ê²€ì¦ í†µê³¼")
        except Exception as e:
            print(f"   âŒ YAML ê²€ì¦ ì‹¤íŒ¨: {e}")
            return None
        
        return str(yaml_path)

    def augment_low_frequency_classes(self, need_augmentation):
        """ì €ë¹ˆë„ í´ë˜ìŠ¤ ë°ì´í„° ì¦ê°•"""
        if not need_augmentation:
            print("âœ… ëª¨ë“  í´ë˜ìŠ¤ê°€ ëª©í‘œì— ë„ë‹¬í•¨")
            return True
        
        print(f"\nğŸ”„ {len(need_augmentation)}ê°œ í´ë˜ìŠ¤ ë°ì´í„° ì¦ê°• ì‹œì‘...")
        
        # ì¦ê°• ì„¤ì •
        transform = A.Compose([
            # ìƒ‰ìƒ ë³´ì¡´ ì¤‘ì‹¬ì˜ ê°€ë²¼ìš´ ì¦ê°•
            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.15, hue=0.05, p=0.6),
            A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=0.4),
            
            # ê¸°í•˜í•™ì  ë³€í™˜
            A.HorizontalFlip(p=0.5),
            A.VerticalFlip(p=0.1),
            A.Rotate(limit=20, border_mode=cv2.BORDER_CONSTANT, value=0, p=0.7),
            A.ShiftScaleRotate(
                shift_limit=0.15, scale_limit=0.25, rotate_limit=15, 
                border_mode=cv2.BORDER_CONSTANT, value=0, p=0.6
            ),
            
            # ê°€ë²¼ìš´ ë…¸ì´ì¦ˆ
            A.OneOf([
                A.GaussNoise(var_limit=(5.0, 20.0)),
                A.GaussianBlur(blur_limit=3),
            ], p=0.3),
            
        ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels'], min_visibility=0.4))
        
        total_created = 0
        
        for class_id, class_name, needed in need_augmentation:
            print(f"\nğŸ“ˆ {class_name} (ID: {class_id}) ì¦ê°• ì¤‘... í•„ìš”: {needed}ê°œ")
            
            # í•´ë‹¹ í´ë˜ìŠ¤ê°€ í¬í•¨ëœ ì´ë¯¸ì§€ ì°¾ê¸°
            source_images = self._find_images_with_class(class_id)
            
            if not source_images:
                print(f"   âŒ ì†ŒìŠ¤ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                continue
            
            print(f"   ğŸ“· ì†ŒìŠ¤ ì´ë¯¸ì§€: {len(source_images)}ê°œ")
            
            created = 0
            max_failures = 100
            failures = 0
            
            # ì¦ê°• ë£¨í”„
            for cycle in range(20):  # ìµœëŒ€ 20 ì‚¬ì´í´
                if created >= needed:
                    break
                
                # í˜„ì¬ ê°œìˆ˜ í™•ì¸ (ì‹¤ì‹œê°„)
                current_count = self._get_current_class_count(class_id)
                if current_count >= self.max_instances:
                    print(f"   ğŸ›‘ ìƒí•œì„  ë„ë‹¬: {current_count}ê°œ")
                    break
                
                for img_path, label_path in source_images:
                    if created >= needed:
                        break
                    
                    try:
                        # ì´ë¯¸ì§€ ë¡œë“œ
                        image = cv2.imread(str(img_path))
                        if image is None:
                            continue
                        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                        
                        # ë¼ë²¨ ë¡œë“œ
                        bboxes, class_labels = self._load_labels(label_path)
                        
                        if class_id not in class_labels:
                            continue
                        
                        # ì¦ê°• ì ìš©
                        augmented = transform(image=image, bboxes=bboxes, class_labels=class_labels)
                        aug_image = augmented['image']
                        aug_bboxes = augmented['bboxes']
                        aug_labels = augmented['class_labels']
                        
                        # íƒ€ê²Ÿ í´ë˜ìŠ¤ê°€ ì—¬ì „íˆ ìˆëŠ”ì§€ í™•ì¸
                        if class_id not in aug_labels:
                            failures += 1
                            if failures > max_failures:
                                print(f"   âš ï¸ ì—°ì† ì‹¤íŒ¨ {max_failures}íšŒ, ì¤‘ë‹¨")
                                break
                            continue
                        
                        # íŒŒì¼ ì €ì¥
                        base_name = img_path.stem
                        aug_name = f"{base_name}_aug_{class_name}_{created:04d}"
                        
                        # ì´ë¯¸ì§€ ì €ì¥
                        aug_img_path = self.dataset_path / "train" / "images" / f"{aug_name}.jpg"
                        aug_image_bgr = cv2.cvtColor(aug_image, cv2.COLOR_RGB2BGR)
                        cv2.imwrite(str(aug_img_path), aug_image_bgr)
                        
                        # ë¼ë²¨ ì €ì¥
                        aug_label_path = self.dataset_path / "train" / "labels" / f"{aug_name}.txt"
                        with open(aug_label_path, 'w') as f:
                            for bbox, label in zip(aug_bboxes, aug_labels):
                                x_center, y_center, bbox_width, bbox_height = bbox
                                f.write(f"{label} {x_center:.6f} {y_center:.6f} {bbox_width:.6f} {bbox_height:.6f}\n")
                        
                        created += 1
                        failures = 0  # ì„±ê³µ ì‹œ ì‹¤íŒ¨ ì¹´ìš´í„° ë¦¬ì…‹
                        
                        if created % 50 == 0:
                            current_total = self._get_current_class_count(class_id)
                            print(f"   ì§„í–‰: {created}/{needed} (í˜„ì¬ ì´: {current_total}ê°œ)")
                        
                    except Exception as e:
                        failures += 1
                        continue
                
                if failures > max_failures:
                    break
            
            final_count = self._get_current_class_count(class_id)
            print(f"   âœ… {class_name} ì™„ë£Œ: {created}ê°œ ìƒì„± (ìµœì¢…: {final_count}ê°œ)")
            total_created += created
        
        print(f"\nğŸ‰ ë°ì´í„° ì¦ê°• ì™„ë£Œ! ì´ ìƒì„±: {total_created}ê°œ")
        return True

    def _find_images_with_class(self, target_class_id):
        """íŠ¹ì • í´ë˜ìŠ¤ê°€ í¬í•¨ëœ ì´ë¯¸ì§€ ì°¾ê¸°"""
        labels_dir = self.dataset_path / "train" / "labels"
        images_dir = self.dataset_path / "train" / "images"
        
        images_with_class = []
        
        for label_file in labels_dir.glob("*.txt"):
            image_file = None
            for ext in ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG']:
                potential_image = images_dir / (label_file.stem + ext)
                if potential_image.exists():
                    image_file = potential_image
                    break
            
            if image_file:
                try:
                    with open(label_file, 'r') as f:
                        for line in f:
                            line = line.strip()
                            if line:
                                parts = line.split()
                                if len(parts) >= 5:
                                    class_id = int(parts[0])
                                    if class_id == target_class_id:
                                        images_with_class.append((image_file, label_file))
                                        break
                except:
                    continue
        
        return images_with_class

    def _load_labels(self, label_path):
        """ë¼ë²¨ íŒŒì¼ì—ì„œ bboxì™€ í´ë˜ìŠ¤ ë¡œë“œ"""
        bboxes = []
        class_labels = []
        
        try:
            with open(label_path, 'r') as f:
                for line in f:
                    line = line.strip()
                    if line:
                        parts = line.split()
                        if len(parts) >= 5:
                            cls = int(parts[0])
                            x_center, y_center, bbox_width, bbox_height = map(float, parts[1:5])
                            bboxes.append([x_center, y_center, bbox_width, bbox_height])
                            class_labels.append(cls)
        except:
            pass
        
        return bboxes, class_labels

    def _get_current_class_count(self, class_id):
        """íŠ¹ì • í´ë˜ìŠ¤ì˜ í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ í™•ì¸"""
        labels_dir = self.dataset_path / "train" / "labels"
        count = 0
        
        for label_file in labels_dir.glob("*.txt"):
            try:
                with open(label_file, 'r') as f:
                    for line in f:
                        line = line.strip()
                        if line:
                            parts = line.split()
                            if len(parts) >= 5 and int(parts[0]) == class_id:
                                count += 1
            except:
                continue
        
        return count

    def analyze_class_distribution(self):
        """í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„"""
        print("\nğŸ“Š í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„ ì¤‘...")
        
        class_counts = Counter()
        
        labels_dir = self.dataset_path / "train" / "labels"
        for label_file in labels_dir.glob("*.txt"):
            try:
                with open(label_file, 'r') as f:
                    for line in f:
                        line = line.strip()
                        if line:
                            parts = line.split()
                            if len(parts) >= 5:
                                class_id = int(parts[0])
                                class_counts[class_id] += 1
            except:
                continue
        
        print(f"\n{'ID':<3} {'í´ë˜ìŠ¤ëª…':<20} {'í˜„ì¬ ìˆ˜':<8} {'ëª©í‘œ':<8} {'ìƒíƒœ':<10}")
        print("-" * 60)
        
        need_augmentation = []
        
        for class_id in range(len(self.valid_classes)):
            class_name = self.valid_classes[class_id]
            current_count = class_counts.get(class_id, 0)
            
            if current_count >= self.max_instances:
                status = "ğŸŸ¢ ìƒí•œì´ˆê³¼"
            elif current_count >= self.target_instances:
                status = "âœ… ëª©í‘œë‹¬ì„±"
            else:
                needed = self.target_instances - current_count
                status = f"ğŸ”´ {needed}ê°œ í•„ìš”"
                need_augmentation.append((class_id, class_name, needed))
            
            print(f"{class_id:<3} {class_name:<20} {current_count:<8} {self.target_instances:<8} {status:<10}")
        
        return need_augmentation
        """í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„"""
        print("\nğŸ“Š í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„ ì¤‘...")
        
        class_counts = Counter()
        
        labels_dir = self.dataset_path / "train" / "labels"
        for label_file in labels_dir.glob("*.txt"):
            try:
                with open(label_file, 'r') as f:
                    for line in f:
                        line = line.strip()
                        if line:
                            parts = line.split()
                            if len(parts) >= 5:
                                class_id = int(parts[0])
                                class_counts[class_id] += 1
            except:
                continue
        
        print(f"\n{'ID':<3} {'í´ë˜ìŠ¤ëª…':<20} {'í˜„ì¬ ìˆ˜':<8} {'ëª©í‘œ':<8} {'ìƒíƒœ':<10}")
        print("-" * 60)
        
        need_augmentation = []
        
        for class_id in range(len(self.valid_classes)):
            class_name = self.valid_classes[class_id]
            current_count = class_counts.get(class_id, 0)
            
            if current_count >= self.max_instances:
                status = "ğŸŸ¢ ìƒí•œì´ˆê³¼"
            elif current_count >= self.target_instances:
                status = "âœ… ëª©í‘œë‹¬ì„±"
            else:
                needed = self.target_instances - current_count
                status = f"ğŸ”´ {needed}ê°œ í•„ìš”"
                need_augmentation.append((class_id, class_name, needed))
            
            print(f"{class_id:<3} {class_name:<20} {current_count:<8} {self.target_instances:<8} {status:<10}")
        
        return need_augmentation

    def safe_train_model(self, yaml_path):
        """ì„œë²„ ê³µìœ  í™˜ê²½ìš© ì•ˆì „í•œ ëª¨ë¸ í›ˆë ¨ (15GB ë©”ëª¨ë¦¬ ìµœì í™”)"""
        print("\nğŸš€ ì„œë²„ ê³µìœ  í™˜ê²½ìš© ì•ˆì „í•œ í›ˆë ¨ ì‹œì‘!")
        
        # YAML íŒŒì¼ ê²€ì¦
        if not Path(yaml_path).exists():
            print(f"âŒ YAML íŒŒì¼ ì—†ìŒ: {yaml_path}")
            return None, None
        
        print(f"ğŸ“„ YAML: {yaml_path}")
        
        # ëª¨ë¸ ë¡œë“œ
        model = YOLO(f'yolo11{self.model_size}.pt')
        
        # GPU ë©”ëª¨ë¦¬ í™•ì¸ ë° ìµœì í™”
        if torch.cuda.is_available():
            device = 0  # ì²« ë²ˆì§¸ GPU
            gpu_memory = torch.cuda.get_device_properties(device).total_memory / (1024**3)
            print(f"ğŸ’» GPU í›ˆë ¨: CUDA:{device} ({gpu_memory:.1f}GB ì¤‘ ~15GB ì‚¬ìš©)")
            torch.cuda.empty_cache()
            
            # ì„œë²„ ê³µìœ  í™˜ê²½ìš© ì•ˆì „í•œ ë°°ì¹˜ í¬ê¸° (15GB ê¸°ì¤€)
            batch_sizes = {
                'n': 64,   # nano: 15GBì—ì„œ ì¶©ë¶„íˆ ì•ˆì „
                's': 32,   # small: ì•ˆì •ì 
                'm': 24,   # medium: 15GBì— ìµœì í™”
                'l': 16,   # large: ë³´ìˆ˜ì 
                'x': 8     # xlarge: ë§¤ìš° ì•ˆì „
            }
            print(f"ğŸ¢ ì„œë²„ ê³µìœ  í™˜ê²½ - ì•ˆì „í•œ ë°°ì¹˜ í¬ê¸° ì‚¬ìš©")
        else:
            device = 'cpu'
            batch_sizes = {'n': 16, 's': 8, 'm': 4, 'l': 2, 'x': 1}
            print(f"ğŸ’» CPU í›ˆë ¨")
        
        batch_size = batch_sizes.get(self.model_size, 16)
        print(f"ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {batch_size} (15GB ë©”ëª¨ë¦¬ ê³ ë ¤)")
        
        try:
            results = model.train(
                data=str(yaml_path),
                epochs=self.epochs,
                imgsz=640,
                device=device,
                batch=batch_size,
                patience=25,
                save=True,
                project='final_snack_detection',
                name=f'yolo11{self.model_size}_final',
                exist_ok=True,
                
                # ì„œë²„ ê³µìœ  í™˜ê²½ìš© ì•ˆì „ ì„¤ì •
                workers=6,      # ì ë‹¹í•œ ì›Œì»¤ ìˆ˜
                cache='disk',   # ë””ìŠ¤í¬ ìºì‹œ (RAM ì ˆì•½)
                amp=True,       # í˜¼í•© ì •ë°€ë„ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
                
                # ìµœì í™”ëœ í›ˆë ¨ ì„¤ì •
                lr0=0.01,
                lrf=0.01,
                momentum=0.937,
                weight_decay=0.0005,
                warmup_epochs=3,
                warmup_momentum=0.8,
                warmup_bias_lr=0.1,
                
                # ì ë‹¹í•œ ë°ì´í„° ì¦ê°• (ì„œë²„ ë¦¬ì†ŒìŠ¤ ê³ ë ¤)
                hsv_h=0.015,
                hsv_s=0.7,
                hsv_v=0.4,
                degrees=0.0,
                translate=0.1,
                scale=0.5,
                shear=0.0,
                perspective=0.0,
                flipud=0.0,
                fliplr=0.5,
                
                mosaic=1.0,
                mixup=0.0,
                copy_paste=0.0,
                
                # ì•ˆì „í•œ ì„¤ì •
                plots=True,
                val=True,
                verbose=True,
                save_period=10,
                
                # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
                multi_scale=False,  # ë©”ëª¨ë¦¬ ì ˆì•½
                rect=False,
                cos_lr=True,
                close_mosaic=10,
                
                # ì†ì‹¤ í•¨ìˆ˜ ê°€ì¤‘ì¹˜
                box=7.5,
                cls=0.5,
                dfl=1.5,
            )
            
            print("\nğŸ‰ í›ˆë ¨ ì™„ë£Œ!")
            
            # ì„±ëŠ¥ í™•ì¸
            try:
                metrics = model.val()
                print(f"\nğŸ“ˆ ìµœì¢… ì„±ëŠ¥:")
                print(f"  mAP50: {metrics.box.map50:.4f} ({metrics.box.map50*100:.1f}%)")
                print(f"  mAP50-95: {metrics.box.map:.4f} ({metrics.box.map*100:.1f}%)")
                print(f"  Precision: {metrics.box.mp:.4f} ({metrics.box.mp*100:.1f}%)")
                print(f"  Recall: {metrics.box.mr:.4f} ({metrics.box.mr*100:.1f}%)")
                
                # ì„±ëŠ¥ í‰ê°€
                if metrics.box.map50 >= 0.7:
                    print(f"ğŸŒŸ ìš°ìˆ˜í•œ ì„±ëŠ¥! mAP50 {metrics.box.map50:.3f}")
                elif metrics.box.map50 >= 0.5:
                    print(f"âœ… ì–‘í˜¸í•œ ì„±ëŠ¥: mAP50 {metrics.box.map50:.3f}")
                else:
                    print(f"âš ï¸ ê°œì„  í•„ìš”: mAP50 {metrics.box.map50:.3f}")
                    
            except Exception as e:
                print(f"âš ï¸ ê²€ì¦ ì˜¤ë¥˜: {e}")
            
            return model, results
            
        except Exception as e:
            print(f"âŒ í›ˆë ¨ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return None, None

    def run_complete_pipeline(self):
        """ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ë°ì´í„° ì¦ê°• í¬í•¨)"""
        print("ğŸ¯ ìµœì¢… ê³¼ì íƒì§€ ëª¨ë¸ íŒŒì´í”„ë¼ì¸")
        print("=" * 50)
        print("1ï¸âƒ£ ë¼ë²¨ ìŠ¤ìº” ë° ìˆ˜ì •")
        print("2ï¸âƒ£ ìµœì¢… ê²€ì¦")
        print("3ï¸âƒ£ YAML ìƒì„±") 
        print("4ï¸âƒ£ í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„")
        print("5ï¸âƒ£ ë°ì´í„° ì¦ê°• (í•„ìš”ì‹œ)")
        print("6ï¸âƒ£ ëª¨ë¸ í›ˆë ¨")
        print("=" * 50)
        
        try:
            # 1ë‹¨ê³„: ë¼ë²¨ ì •ë¦¬
            print("\n1ï¸âƒ£ ë¼ë²¨ ìŠ¤ìº” ë° ìˆ˜ì •...")
            if not self.scan_and_fix_all_labels():
                print("âŒ ë¼ë²¨ ì •ë¦¬ ì‹¤íŒ¨")
                return None, None
            
            # 2ë‹¨ê³„: ê²€ì¦
            print("\n2ï¸âƒ£ ìµœì¢… ê²€ì¦...")
            if not self.validate_final_labels():
                print("âŒ ë¼ë²¨ ê²€ì¦ ì‹¤íŒ¨")
                return None, None
            
            # 3ë‹¨ê³„: YAML ìƒì„±
            print("\n3ï¸âƒ£ YAML ìƒì„±...")
            yaml_path = self.create_final_yaml()
            if not yaml_path:
                print("âŒ YAML ìƒì„± ì‹¤íŒ¨")
                return None, None
            
            # 4ë‹¨ê³„: ë¶„í¬ ë¶„ì„
            print("\n4ï¸âƒ£ í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„...")
            need_aug = self.analyze_class_distribution()
            
            # 5ë‹¨ê³„: ë°ì´í„° ì¦ê°• (í•„ìš”ì‹œ)
            if need_aug:
                print(f"\n5ï¸âƒ£ ë°ì´í„° ì¦ê°•...")
                print(f"ğŸ“ˆ {len(need_aug)}ê°œ í´ë˜ìŠ¤ê°€ ëª©í‘œ ë¯¸ë‹¬")
                
                proceed_aug = input("ë°ì´í„° ì¦ê°•ì„ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
                if proceed_aug in ['y', 'yes']:
                    if not self.augment_low_frequency_classes(need_aug):
                        print("âŒ ë°ì´í„° ì¦ê°• ì‹¤íŒ¨")
                        return None, None
                    
                    # ì¦ê°• í›„ ì¬ë¶„ì„
                    print("\nğŸ“Š ì¦ê°• í›„ ë¶„í¬ ì¬ë¶„ì„...")
                    final_need_aug = self.analyze_class_distribution()
                    if final_need_aug:
                        print(f"âš ï¸ ì—¬ì „íˆ {len(final_need_aug)}ê°œ í´ë˜ìŠ¤ê°€ ëª©í‘œ ë¯¸ë‹¬")
                else:
                    print("â­ï¸ ë°ì´í„° ì¦ê°• ê±´ë„ˆëœ€")
            else:
                print("âœ… ëª¨ë“  í´ë˜ìŠ¤ê°€ ëª©í‘œì— ë„ë‹¬!")
            
            # 6ë‹¨ê³„: í›ˆë ¨
            print(f"\n6ï¸âƒ£ ëª¨ë¸ í›ˆë ¨...")
            model, results = self.safe_train_model(yaml_path)
            
            if model and results:
                print(f"\nğŸŠ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
                print(f"ğŸ“ ê²°ê³¼: final_snack_detection/yolo11{self.model_size}_final/")
                print(f"ğŸ† ëª¨ë¸: weights/best.pt")
                print(f"ğŸ“Š ë¡œê·¸: results.csv")
                print(f"ğŸ“ˆ ê·¸ë˜í”„: results.png")
            
            return model, results
            
        except Exception as e:
            print(f"âŒ íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return None, None


# ì‹¤í–‰ë¶€
if __name__ == "__main__":
    print("ğŸ¯ ìµœì¢… ê³¼ì íƒì§€ ëª¨ë¸ í›ˆë ¨ê¸°")
    print("=" * 50)
    
    # ì„¤ì •
    dataset_path = "snack_data_2"
    
    print(f"ğŸ“‚ ë°ì´í„°ì…‹: {dataset_path}")
    print(f"ğŸ¯ ëª©í‘œ: 600ê°œ/í´ë˜ìŠ¤")
    print(f"ğŸ¤– ëª¨ë¸: YOLO11m")
    print(f"ğŸ”„ ì—í¬í¬: 80")
    print(f"ğŸš« ì œì™¸: Backside")
    
    # ê²½ë¡œ í™•ì¸
    if not Path(dataset_path).exists():
        print(f"âŒ ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {dataset_path}")
        print("ì˜¬ë°”ë¥¸ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”:")
        dataset_path = input("ë°ì´í„°ì…‹ ê²½ë¡œ: ").strip() or dataset_path
    
    # í™•ì¸
    proceed = input("\nì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
    if proceed not in ['y', 'yes']:
        print("âŒ ì·¨ì†Œë¨")
        exit(0)
    
    # ì‹¤í–‰
    trainer = FinalSnackTrainer(
        dataset_path=dataset_path,
        target_instances=600,
        model_size='m',
        epochs=80,
        exclude_classes=[1]  # Backside ì œì™¸
    )
    
    model, results = trainer.run_complete_pipeline()
    
    if model and results:
        print(f"\nğŸ‰ ì„±ê³µ!")
        print(f"ğŸ“ final_snack_detection/yolo11m_final/")
    else:
        print(f"âŒ ì‹¤íŒ¨")